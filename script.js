// script.js - ThamAI Chat Ultra+ v1.2
const API_BASE = "https://thamai-backend-new.onrender.com"; // <-- ƒë·ªïi th√†nh URL backend c·ªßa anh

/* -----------------------
   DOM
-----------------------*/
const messagesEl = document.getElementById("messages");
const userInput = document.getElementById("user-input");
const sendBtn = document.getElementById("send-btn");
const recordBtn = document.getElementById("record-btn");
const speakBtn = document.getElementById("speak-btn");
const settingsBtn = document.getElementById("settings-btn");
const settingsPanel = document.getElementById("settings-panel");
const voiceSelect = document.getElementById("voice-select");
const langSelect = document.getElementById("lang-select");
const testVoiceBtn = document.getElementById("test-voice");
const saveSettingsBtn = document.getElementById("save-settings");
const retryBtn = document.getElementById("retry-btn");
const backendStatus = document.getElementById("backend-status");
const statusText = document.getElementById("status-text");
const statusIcon = document.getElementById("status-icon");
const audioPlayer = document.getElementById("audio-player");
const tingAudio = document.getElementById("ting-audio");
const waveCanvas = document.getElementById("wave-canvas");
const mouthEl = document.getElementById("mouth");
const avatarImg = document.getElementById("avatar-img");
const tingSound = new Audio('assets/ting.mp3');
const avatarNormal = 'assets/avatar.png';
const avatarSmile = 'assets/avatar_smile.png';
const waveGif = 'assets/wave.gif';

let mediaRecorder = null;
let audioChunks = [];
let lastBotReply = "";
let audioCtx = null;
let analyser = null;
let sourceNode = null;
let animationId = null;
let isRecording = false;

/* -----------------------
   Utility
-----------------------*/
function appendMessage(sender, text) {
  const el = document.createElement("div");
  el.className = `message ${sender === "user" ? "user" : "bot"}`;
  el.textContent = text;
  messagesEl.appendChild(el);
  messagesEl.scrollTop = messagesEl.scrollHeight;
}

/* -----------------------
   Backend check with pulse + ping sound
-----------------------*/
async function updateStatus(connected) {
  if (connected) {
    backendStatus.classList.remove("disconnected");
    backendStatus.classList.add("connected");
    statusText.textContent = "Backend ThamAI ho·∫°t ƒë·ªông t·ªët!";
    statusIcon.textContent = "‚úÖ";
    // ping sound
    try {
      await tingAudio.play();
    } catch (e) {
      // autoplay locked ‚Äî ignore
    }
    // avatar pulse
    avatarImg.style.transform = "scale(1.02)";
    setTimeout(()=> avatarImg.style.transform = "", 500);
  } else {
    backendStatus.classList.remove("connected");
    backendStatus.classList.add("disconnected");
    statusText.textContent = "Kh√¥ng th·ªÉ k·∫øt n·ªëi m√°y ch·ªß backend.";
    statusIcon.textContent = "‚ö†Ô∏è";
  }
}

async function checkBackend() {
  backendStatus.classList.add("checking");
  statusText.textContent = "ƒêang ki·ªÉm tra...";
  statusIcon.textContent = "‚è≥";
  try {
    const res = await fetch(`${API_BASE}/test`);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    const data = await res.json();
    const ok = data && (data.status === "ok" || data.message);
    await updateStatus(Boolean(ok));
    return ok;
  } catch (err) {
    console.error("L·ªói k·∫øt n·ªëi:", err);
    await updateStatus(false);
    return false;
  } finally {
    backendStatus.classList.remove("checking");
  }
}
retryBtn.addEventListener("click", () => checkBackend());

checkBackend(); // on load

/* -----------------------
   Persist settings (localStorage)
-----------------------*/
const SETTINGS_KEY = "thamai_settings_v1";
function loadSettings() {
  try {
    const raw = localStorage.getItem(SETTINGS_KEY);
    if (!raw) return;
    const s = JSON.parse(raw);
    if (s.voice) voiceSelect.value = s.voice;
    if (s.lang) langSelect.value = s.lang;
    if (s.autoplay !== undefined) document.getElementById("autoplay-tts").checked = s.autoplay;
  } catch (e) { console.warn("Load settings failed", e); }
}
function saveSettings() {
  const s = {
    voice: voiceSelect.value,
    lang: langSelect.value,
    autoplay: document.getElementById("autoplay-tts").checked
  };
  localStorage.setItem(SETTINGS_KEY, JSON.stringify(s));
  appendMessage("bot", "‚öôÔ∏è ƒê√£ l∆∞u c√†i ƒë·∫∑t gi·ªçng.");
}
saveSettingsBtn.addEventListener("click", saveSettings);
loadSettings();

/* -----------------------
   Waveform + mouth animation setup
-----------------------*/
function createAudioContext() {
  if (audioCtx) return audioCtx;
  audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  analyser = audioCtx.createAnalyser();
  analyser.fftSize = 2048;
  return audioCtx;
}

function attachSourceFromMediaElement(el) {
  createAudioContext();
  try {
    if (sourceNode) sourceNode.disconnect();
    sourceNode = audioCtx.createMediaElementSource(el);
    sourceNode.connect(analyser);
    analyser.connect(audioCtx.destination); // to allow audio to play through page
  } catch (e) {
    console.warn("attachSourceFromMediaElement failed:", e);
  }
}

function attachSourceFromStream(stream) {
  createAudioContext();
  try {
    if (sourceNode) sourceNode.disconnect();
    sourceNode = audioCtx.createMediaStreamSource(stream);
    sourceNode.connect(analyser);
    // do not connect analyser to destination for mic (we don't want feedback)
  } catch (e) {
    console.warn("attachSourceFromStream failed:", e);
  }
}

function startVisuals() {
  const canvas = waveCanvas;
  canvas.width = canvas.clientWidth * devicePixelRatio;
  canvas.height = canvas.clientHeight * devicePixelRatio;
  const ctx = canvas.getContext("2d");
  ctx.scale(devicePixelRatio, devicePixelRatio);

  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);

  function draw() {
    animationId = requestAnimationFrame(draw);
    analyser.getByteTimeDomainData(dataArray);
    ctx.clearRect(0, 0, canvas.clientWidth, canvas.clientHeight);

    // waveform
    ctx.lineWidth = 2;
    ctx.strokeStyle = "rgba(9,161,122,0.8)";
    ctx.beginPath();
    const sliceWidth = canvas.clientWidth / bufferLength;
    let x = 0;
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
      const v = dataArray[i] / 128.0 - 1.0;
      const y = (canvas.clientHeight / 2) + v * (canvas.clientHeight / 3);
      if (i === 0) ctx.moveTo(x, y);
      else ctx.lineTo(x, y);
      x += sliceWidth;
      sum += Math.abs(v);
    }
    ctx.stroke();

    // mouth animation: compute RMS-like value
    const rms = sum / bufferLength;
    const mouthScale = Math.min(1.8, 1 + rms * 10);
    mouthEl.style.transform = `scaleY(${Math.max(0.6, mouthScale)})`;
    if (rms > 0.06) mouthEl.classList.add("smile"); else mouthEl.classList.remove("smile");

    // subtle avatar brightness
    avatarImg.style.filter = `brightness(${1 + Math.min(0.2, rms)})`;
  }

  if (!animationId) draw();
}

function stopVisuals() {
  if (animationId) cancelAnimationFrame(animationId);
  animationId = null;
  const canvas = waveCanvas;
  const ctx = canvas.getContext("2d");
  ctx.clearRect(0,0,canvas.clientWidth, canvas.clientHeight);
  mouthEl.style.transform = "";
  avatarImg.style.filter = "";
}

/* -----------------------
   Chat: /chat
-----------------------*/
sendBtn.addEventListener("click", async () => {
  const message = userInput.value.trim();
  if (!message) return;
  appendMessage("user", message);
  userInput.value = "";
  try {
    const res = await fetch(`${API_BASE}/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ message })
    });
    if (!res.ok) {
      appendMessage("bot", `‚ö†Ô∏è L·ªói chat HTTP ${res.status}`);
      return;
    }
    const data = await res.json();
    if (data.reply) {
      appendMessage("bot", data.reply);
      lastBotReply = data.reply;
      if (document.getElementById("autoplay-tts").checked) speakBtn.click();
    } else {
      appendMessage("bot", "‚ùå Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ backend.");
    }
  } catch (e) {
    console.error("Chat fetch failed:", e);
    appendMessage("bot", "‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi m√°y ch·ªß backend.");
  }
});

/* -----------------------
   Whisper (recording) -> /whisper
   - when recording stops: send file, backend returns JSON {text}
   - attach mic stream to analyser for live visuals
-----------------------*/
recordBtn.addEventListener("click", async () => {
  try {
    if (isRecording) {
      // stop
      mediaRecorder.stop();
      isRecording = false;
      recordBtn.textContent = "üé§ Ghi √¢m";
      stopVisuals();
      if (audioCtx && audioCtx.state !== "closed") audioCtx.suspend().catch(()=>{});
      return;
    }

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    attachSourceFromStream(stream);
    startVisuals();

    // choose supported mimeType
    let mime = "";
    if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) mime = "audio/webm;codecs=opus";
    else if (MediaRecorder.isTypeSupported("audio/webm")) mime = "audio/webm";
    else if (MediaRecorder.isTypeSupported("audio/wav")) mime = "audio/wav";

    mediaRecorder = new MediaRecorder(stream, mime ? { mimeType: mime } : undefined);
    audioChunks = [];

    mediaRecorder.ondataavailable = (e) => { if (e.data && e.data.size > 0) audioChunks.push(e.data); };

    mediaRecorder.onstop = async () => {
      const blob = new Blob(audioChunks, { type: audioChunks[0]?.type || "audio/webm" });
      appendMessage("user", "üéôÔ∏è (ƒêang g·ª≠i file ghi √¢m...)");

      const fd = new FormData();
      fd.append("file", blob, "record.webm");

      try {
        const res = await fetch(`${API_BASE}/whisper`, { method: "POST", body: fd });
        if (!res.ok) {
          const txt = await res.text();
          console.error("Whisper HTTP error", res.status, txt);
          appendMessage("bot", `‚ö†Ô∏è Whisper l·ªói HTTP (${res.status}).`);
          return;
        }
        const data = await res.json();
        if (data.text) {
          appendMessage("user", "üó£Ô∏è " + data.text);
          userInput.value = data.text;
        } else {
          appendMessage("bot", "‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c gi·ªçng n√≥i.");
          console.error("Whisper unexpected:", data);
        }
      } catch (err) {
        console.error("Whisper fetch failed:", err);
        appendMessage("bot", "‚ö†Ô∏è L·ªói khi g·ª≠i file ghi √¢m.");
      }
    };

    mediaRecorder.start();
    isRecording = true;
    recordBtn.textContent = "‚èπÔ∏è D·ª´ng";

    // resume audio context if suspended
    if (audioCtx && audioCtx.state === "suspended") audioCtx.resume().catch(()=>{});
  } catch (e) {
    console.error("Ghi √¢m failed:", e);
    appendMessage("bot", "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p micro: " + (e.message || e));
  }
});

/* -----------------------
   TTS speak -> /speak
   - get blob audio and play via audio element
   - attach audio element to analyser to show waveform
-----------------------*/
speakBtn.addEventListener("click", async () => {
  const text = lastBotReply || userInput.value.trim();
  if (!text) {
    appendMessage("bot", "Ch∆∞a c√≥ n·ªôi dung ƒë·ªÉ ThamAI n√≥i.");
    return;
  }

  try {
    const gender = voiceSelect.value || "female";
    const lang = langSelect.value || "vi";
    const res = await fetch(`${API_BASE}/speak`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ text, gender, lang })
    });

    if (!res.ok) {
      const txt = await res.text();
      console.error("TTS HTTP error:", res.status, txt);
      appendMessage("bot", "‚ö†Ô∏è L·ªói khi y√™u c·∫ßu ph√°t √¢m thanh.");
      return;
    }

    const blob = await res.blob();
    if (!blob.type.startsWith("audio")) {
      const txt = await blob.text();
      console.error("Ph·∫£n h·ªìi TTS kh√¥ng ph·∫£i √¢m thanh:", txt);
      appendMessage("bot", "‚ö†Ô∏è M√°y ch·ªß ch∆∞a tr·∫£ v·ªÅ √¢m thanh h·ª£p l·ªá.");
      return;
    }

    const url = URL.createObjectURL(blob);
    audioPlayer.src = url;
    audioPlayer.hidden = false;

    // attach analyser to this audio element to visualize TTS
    try {
      // create audio context and attach
      createAudioContext();
      attachSourceFromMediaElement(audioPlayer);
      startVisuals();
      audioPlayer.play().catch(e => console.warn("Audio play prevented:", e));
      // when audio ends, stop visuals
      audioPlayer.onended = () => {
        stopVisuals();
        if (audioCtx && audioCtx.state !== "suspended") audioCtx.suspend().catch(()=>{});
      };
    } catch (e) {
      console.warn("Attach TTS to visuals failed:", e);
    }

  } catch (err) {
    console.error("TTS fetch failed:", err);
    appendMessage("bot", "‚ö†Ô∏è Kh√¥ng th·ªÉ ph√°t √¢m thanh.");
  }
});

/* -----------------------
   Settings UI
-----------------------*/
settingsBtn.addEventListener("click", ()=> settingsPanel.classList.toggle("hidden"));
testVoiceBtn.addEventListener("click", async () => {
  // quick test phrase
  lastBotReply = (langSelect.value === "en") ? "Hello, this is ThachAI speaking." : "Xin ch√†o, ƒë√¢y l√† b·∫£n th·ª≠ gi·ªçng Th·∫°chAI.";
  await speakBtn.click();
});

/* enable Enter key to send */
userInput.addEventListener("keydown", (e) => {
  if (e.key === "Enter" && !e.shiftKey) {
    e.preventDefault();
    sendBtn.click();
  }
});

/* -----------------------
   On load: set UI from saved settings
-----------------------*/
document.addEventListener("DOMContentLoaded", () => {
  loadSettings();
});
